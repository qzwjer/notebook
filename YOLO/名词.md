
>confusion matrix 混淆矩阵，在机器学习和统计学中，用于评估分类模型性能的矩阵。它显示了模型预测结果与实际标签之间的对应关系，可以用于计算准确率、召回率、精确率等指标。

> YOLO you only look once

> bounding box 边界框，几何学中用于包围点集的最小框，可以测量点集的面积、体积或超体积。计算机视觉算法通过识别边界框来准确检测物体。

> Region Proposal 区域提案，在计算机视觉中，指的是一种用于目标检测和图像分割的算法，通过生成候选区域来提供可能包含目标的图像区域。

> ground truth 地面实况：通过直接观察获得的信息，而不是通过推断获得的信息。研究人员使用地面实况数据来验证他们的模型的准确性。
> 整个ground truth 为：
> $S * S * (B * 5 + C)$
> S * S：grid 的数量
> B：每个框预测的bounding box 的数量
> 5：bounding box的量，分别是中心位置(x,y)，高(h)，宽(w)，以及置信度
> C：分类器可以识别的物体数

> IOU intersection over union，交并比，在检测任务中，使用交并比(Intersection of Union，IOU)作为衡量指标,来描述两个框之间的重合度。 $IOU = \frac{A \cap B}{A \cup B} $


> confidence 置信度 $C = Pr(obj) * IOU^{pred}_{truth}$

> one-hot  一位有效编码：一种编码方式，其中一个向量的所有元素都为0，除了一个元素为1。这种编码常用于表示类别变量，如机器学习中的类别标签。

> 非极大值抑制 Non-maximum suppression(NMS)
> Algorithm 1 Non-Maximum Suppression Algorithm
```
Require: Set of predicted bounding boxes B, confidence scores S, IoU threshold τ , confidence threshold T
Ensure: Set of filtered bounding boxes F
1: F ← ∅
2: Filter the boxes: B ← {b ∈ B | S(b) ≥ T}
3: Sort the boxes B by their confidence scores in descending order
4: while B ̸= ∅ do
5:      Select the box b with the highest confidence score
6:      Add b to the set of final boxes F: F ← F ∪ {b}
7:      Remove b from the set of boxes B: B ← B − {b}
8:      for all remaining boxes r in B do
9:          Calculate the IoU between b and r: iou ← IoU(b, r)
10:         if iou ≥ τ then
11:             Remove r from the set of boxes B: B ← B − {r}
12:         end if
13:     end for
14: end while
```


>MSE mean square error 均方误差

>precision 检测出来的条目中有多大比例是我们需要的

>recall 所有我们需要的条目有多大比例被检测出来了

>PR曲线 我们希望precision 和 recall 越大越好，但两个在某些情况是矛盾的

>AP average precision 平均精度 简单来说就是对PR曲线上的Precision值求均值 $AP=\int_0^1{p(r)dr}$

>mAP mean average precision，对coco数据集，mAP是AP值在所有类别下的均值。在这里，在coco的语境下AP便是mAP，这里的AP已经计算了所有类别下的平均值，这里的AP便是mAP。

>COCO common objects in context

>BP back-propagation 反向传播

>CBL Conv+Bn+Leaky_relu

>CSP  Cross Stage Partial，其初衷是减少计算量并且增强梯度的表现
>论文：[CSPNet: A New Backbone that can Enhance Learning Capability of CNN](https://arxiv.org/pdf/1911.11929.pdf) 

>ResNet Residual Neural Network 残差神经网络
>论文：[Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)
>残差神经网络的主要贡献是发现了“退化现象（Degradation）”，并针对退化现象发明了 “快捷连接（Shortcuts, skip connection）”，有助于训练过程中梯度的反向传播，极大的消除了深度过大的神经网络训练困难问题。神经网络的“深度”首次突破了100层、最大的神经网络甚至超过了1000层。

>DenseNet 的基本思路与ResNet一致，但是它建立的是前面所有层与后面层的密集连接（dense connection），它的名称也是由此而来。
>DenseNet的另一大特色是通过特征在channel上的连接来实现特征重用（feature reuse）。这些特点让DenseNet在参数和计算成本更少的情形下实现比ResNet更优的性能，DenseNet也因此斩获CVPR 2017的最佳论文奖。

>SPP 

>Concat  concatenate

>Conv convolution 卷积和多项式相乘 ，Conv模块是卷积神经网络中常用的一种基础模块，它主要由卷积层、BN层和激活函数组成。 Conv = Conv2d + BN + SILU

>BN BN层是在卷积层之后加入的一种归一化层，用于规范化神经网络中的特征值分布

>Leaky relu  leaky 有漏洞的  relu 线性整流函数（Rectified Linear Unit）

>激活函数 是一种非线性函数，用于给神经网络引入非线性变换能力。常用的激活函数包括sigmoid、ReLU、LeakyReLU、ELU等。它们在输入值的不同范围内都有不同的输出表现，可以更好地适应不同类型的数据分布。

>res unit 

>Focus

>slice

>maxpool

>indice 指数

>transpse 转置

>CNN convolutional neural network

>tensor 多维张量

>FPN feature pyramid network 特征金字塔网络

>SOTA state-of-the-art 最先进的

>FP false positive 假阳性，在检测或诊断过程中，错误地将不存在的问题或病状判断为存在的现象。

>SGD stochastic gradient descent 随机梯度下降

>HOG Histogram of oriented gradients 方向梯度直方图

>k-means clustering k均值聚类，它的基本思想是，通过迭代寻找K个簇（Cluster）的一种划分方案，使得聚类结果对应的损失函数最小，其中，损失函数可以定义为各个样本距离所属簇中心点的误差平方和。参考：<https://zhuanlan.zhihu.com/p/184686598>

>softmax Softmax从字面上来说，可以分成soft和max两个部分。max故名思议就是最大值的意思。
>对于max我们都很熟悉，比如有两个变量a,b。如果a>b，则max为a，反之为b。用伪码简单描述一下就是 if a > b return a; else b。
>另外一个单词为soft。max存在的一个问题是什么呢？如果将max看成一个分类问题，就是非黑即白，最后的输出是一个确定的变量。更多的时候，我们希望输出的是取到某个分类的概率，或者说，我们希望分值大的那一项被经常取到，而分值较小的那一项也有一定的概率偶尔被取到，所以我们就应用到了soft的概念，即最后的输出是每个分类被取到的概率。
>下面给出Softmax函数的定义（以第i个节点输出为例）：
$Softmax(z_i)=\frac{e^{z_i}}{\sum_{c=1}^{C}{e^{z_c}}}$
 >，其中,$z_i$为第i个节点的输出值，C为输出节点的个数，即分类的类别个数。通过Softmax函数就可以将多分类的输出值转换为范围在[0, 1]和为1的概率分布。
 >参考：<https://blog.csdn.net/bitcarmanlee/article/details/82320853>

>sigmoid函数，Sigmoid函数是理解神经网络如何学习复杂问题的关键。这个函数也是学习其他函数的基础，这些函数可以为深度学习架构中的监督学习提供高效的解决方案。它是一个非线性S形函数，它的域是所有实数的集合，范围是(0,1)。因此作为神经元的激活函数，这个单元的输出保证总是介于0和1之间。参考：<https://zhuanlan.zhihu.com/p/424858561>

>hyperparameter 超参，在机器学习的上下文中，超参数是在开始学习过程之前设置值的参数，而不是通过训练得到的参数数据。通常情况下，需要对超参数进行优化，给学习机选择一组最优超参数，以提高学习的性能和效果

>Backbone，主干，在ImageNet上预先训练的网络用来提取特征。
>运行在GPU上的检测器的主干如VGG，ResNet，ResNetXt，DenseNet
>运行在CPU上的检测器的主干如SqueezeNet、MobileNet或ShufflfleNet

>Head，用于预测物体的类和边界框，Dense Prediction, Sparse Prediction，通常可分为一级目标探测器和两级目标探测器两类。
>两级目标探测器是R-CNN系列，包括Fast R-CNN、Faster R-CNN、R-FCN和Libra R-CNN
>也可以使一个两级目标检测器成为一个无锚点的目标检测器，如反应点。
>单级目标探测器，最具代表性的模型是YOLO、SSD和RetinaNet。
>近年来，无锚的单级目标探测器已经发展起来。这类检测器有CenterNet、CornerNet、FCOS等。
>参考论文：[YOLOv4：Optimal Speed and Accuracy of Object Detection](https://readpaper.com/pdf-annotate/note?pdfId=4500345772876390401&noteId=2010198285949619456)

>Neck，近年来开发的目标探测器经常在主干和头部之间插入一些层，这些层通常用于收集不同阶段的特征图，将图像特征传递到预测层。
>通常，颈部由几条自下向上的路径和几条自上向下的路径组成。
>配备这种机制的网络包括特征金字塔网络(FPN)、路径聚合网络(PAN)、BiFPN和NAS-FPN

>Object detector，一个普通的物体探测器由“特征输入、骨干网络、颈部和头部”组成

>RF,receptive field 感受野 接受域

>cross entropy 交叉熵，多分类问题中，经常使用交叉熵作为损失函数

>VGG VGG是Oxford的Visual Geometry Group的组提出的（大家应该能看出VGG名字的由来了）。该网络是在ILSVRC 2014上的相关工作，主要工作是证明了增加网络的深度能够在一定程度上影响网络最终的性能。
>VGG有两种结构，分别是VGG16和VGG19，两者并没有本质上的区别，只是网络深度不一样。
>论文：[Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)
>参考：[一文读懂VGG网络](https://zhuanlan.zhihu.com/p/41423739)

>SVM support vector machines 支持向量机